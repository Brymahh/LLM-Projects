{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-09T10:18:52.096282Z","iopub.execute_input":"2024-07-09T10:18:52.097021Z","iopub.status.idle":"2024-07-09T10:18:54.180298Z","shell.execute_reply.started":"2024-07-09T10:18:52.096965Z","shell.execute_reply":"2024-07-09T10:18:54.179259Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"pip install -U transformers accelerate bitsandbytes sentencepiece","metadata":{"execution":{"iopub.status.busy":"2024-07-09T10:18:54.182058Z","iopub.execute_input":"2024-07-09T10:18:54.182591Z","iopub.status.idle":"2024-07-09T10:19:29.747564Z","shell.execute_reply.started":"2024-07-09T10:18:54.182556Z","shell.execute_reply":"2024-07-09T10:19:29.746502Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\nCollecting transformers\n  Downloading transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m641.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\nCollecting accelerate\n  Downloading accelerate-0.32.1-py3-none-any.whl.metadata (18 kB)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.2.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.2)\nRequirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading transformers-4.42.3-py3-none-any.whl (9.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading accelerate-0.32.1-py3-none-any.whl (314 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes, accelerate, transformers\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.30.1\n    Uninstalling accelerate-0.30.1:\n      Successfully uninstalled accelerate-0.30.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.41.2\n    Uninstalling transformers-4.41.2:\n      Successfully uninstalled transformers-4.41.2\nSuccessfully installed accelerate-0.32.1 bitsandbytes-0.43.1 transformers-4.42.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"!rm -f minsearch.py\n!wget https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py","metadata":{"execution":{"iopub.status.busy":"2024-07-09T11:08:41.762897Z","iopub.execute_input":"2024-07-09T11:08:41.763732Z","iopub.status.idle":"2024-07-09T11:08:43.934549Z","shell.execute_reply.started":"2024-07-09T11:08:41.763699Z","shell.execute_reply":"2024-07-09T11:08:43.933421Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"--2024-07-09 11:08:43--  https://raw.githubusercontent.com/alexeygrigorev/minsearch/main/minsearch.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.111.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3832 (3.7K) [text/plain]\nSaving to: 'minsearch.py'\n\nminsearch.py        100%[===================>]   3.74K  --.-KB/s    in 0s      \n\n2024-07-09 11:08:43 (52.4 MB/s) - 'minsearch.py' saved [3832/3832]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the documents, create the index\n\nimport requests \nimport minsearch\n\ndocs_url = 'https://github.com/DataTalksClub/llm-zoomcamp/blob/main/01-intro/documents.json?raw=1'\ndocs_response = requests.get(docs_url)\ndocuments_raw = docs_response.json()\n\ndocuments = []\n\nfor course in documents_raw:\n    course_name = course['course']\n\n    for doc in course['documents']:\n        doc['course'] = course_name\n        documents.append(doc)\n\nindex = minsearch.Index(\n    text_fields=[\"question\", \"text\", \"section\"],\n    keyword_fields=[\"course\"]\n)\n\nindex.fit(documents)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-09T11:08:43.936930Z","iopub.execute_input":"2024-07-09T11:08:43.937344Z","iopub.status.idle":"2024-07-09T11:08:44.582147Z","shell.execute_reply.started":"2024-07-09T11:08:43.937304Z","shell.execute_reply":"2024-07-09T11:08:44.581220Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"<minsearch.Index at 0x7b9e14ed8c40>"},"metadata":{}}]},{"cell_type":"code","source":"# Define the search Query, specifying the course\n\ndef search(query):\n    boost = {'question': 3.0, 'section': 0.5}\n\n    results = index.search(\n        query=query,\n        filter_dict={'course': 'data-engineering-zoomcamp'},\n        boost_dict=boost,\n        num_results=5\n    )\n\n    return results","metadata":{"execution":{"iopub.status.busy":"2024-07-09T11:08:44.583361Z","iopub.execute_input":"2024-07-09T11:08:44.583732Z","iopub.status.idle":"2024-07-09T11:08:44.589656Z","shell.execute_reply.started":"2024-07-09T11:08:44.583697Z","shell.execute_reply":"2024-07-09T11:08:44.588711Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# prompt with OpenAI gpt3.5-turbo Template\n\n\ndef build_prompt(query, search_results):\n    prompt_template = \"\"\"\nYou're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\nUse only the facts from the CONTEXT when answering the QUESTION.\n\nQUESTION: {question}\n\nCONTEXT: \n{context}\n\"\"\".strip()\n\n    context = \"\"\n    \n    for doc in search_results:\n        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n    \n    prompt = prompt_template.format(question=query, context=context).strip()\n    return prompt\n\ndef llm(prompt):\n    response = client.chat.completions.create(\n        model='gpt-4o',\n        messages=[{\"role\": \"user\", \"content\": prompt}]\n    )\n    \n    return response.choices[0].message.content","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 1. [Flan-T5]('https://huggingface.co/google/flan-t5-xl')","metadata":{}},{"cell_type":"code","source":"# pip install accelerate\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\n","metadata":{"execution":{"iopub.status.busy":"2024-07-09T10:19:42.443922Z","iopub.execute_input":"2024-07-09T10:19:42.444320Z","iopub.status.idle":"2024-07-09T10:19:49.611987Z","shell.execute_reply.started":"2024-07-09T10:19:42.444290Z","shell.execute_reply":"2024-07-09T10:19:49.611221Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\")\nmodel = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\", device_map=\"auto\")","metadata":{"execution":{"iopub.status.busy":"2024-07-09T10:20:14.008866Z","iopub.execute_input":"2024-07-09T10:20:14.009433Z","iopub.status.idle":"2024-07-09T10:21:23.055282Z","shell.execute_reply.started":"2024-07-09T10:20:14.009400Z","shell.execute_reply":"2024-07-09T10:21:23.054384Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6698ab9fad6f4246be07f9eedad8c7ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc351cc04c1d4092876ded83ecd5bbcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b662f5283dc47f983c4b595715e2fb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9416d9ad514948fdb8f570956366b743"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b0eced108e642efabbbf66ca5c3830f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/53.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdd5e273cbeb475d809a50dd4c288617"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c0dff01436b460982530183ef8c3213"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.45G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c2ef510fbd548879b59efc4387c34b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d24a57b05b1b4a2099ebda67493f1b8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9bed4b61aef4e3ca142f9b72831728e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e06482468cb342abb06573465f4b9ac5"}},"metadata":{}}]},{"cell_type":"code","source":"input_text = \"translate English to German: How old are you?\"\ninput_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\")\n\noutputs = model.generate(input_ids)\nprint(tokenizer.decode(outputs[0]))\n","metadata":{"execution":{"iopub.status.busy":"2024-07-09T10:22:21.017277Z","iopub.execute_input":"2024-07-09T10:22:21.017663Z","iopub.status.idle":"2024-07-09T10:22:41.385720Z","shell.execute_reply.started":"2024-07-09T10:22:21.017631Z","shell.execute_reply":"2024-07-09T10:22:41.384742Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"2024-07-09 10:22:26.702217: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-09 10:22:26.702327: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-09 10:22:26.988169: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1249: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"<pad> Wie alt sind Sie?</s>\n","output_type":"stream"}]},{"cell_type":"code","source":"# prompt template with LLM\n\ndef build_prompt(query, search_results):\n    prompt_template = \"\"\"\nYou're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\nUse only the facts from the CONTEXT when answering the QUESTION.\n\nQUESTION: {question}\n\nCONTEXT: \n{context}\n\"\"\".strip()\n\n    context = \"\"\n    \n    for doc in search_results:\n        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n    \n    prompt = prompt_template.format(question=query, context=context).strip()\n    return prompt\n\ndef llm(prompt):\n    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n    outputs = model.generate(input_ids, max_length=100)\n    results = tokenizer.decode(outputs[0])\n    return results","metadata":{"execution":{"iopub.status.busy":"2024-07-09T10:27:37.204646Z","iopub.execute_input":"2024-07-09T10:27:37.205569Z","iopub.status.idle":"2024-07-09T10:27:37.212014Z","shell.execute_reply.started":"2024-07-09T10:27:37.205530Z","shell.execute_reply":"2024-07-09T10:27:37.211175Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# Combining all to RAG Query\n\ndef rag_t5(query):\n    search_results = search(query)\n    prompt = build_prompt(query, search_results)\n    answer = llm(prompt)\n    return answer","metadata":{"execution":{"iopub.status.busy":"2024-07-09T10:40:09.669289Z","iopub.execute_input":"2024-07-09T10:40:09.670123Z","iopub.status.idle":"2024-07-09T10:40:09.674842Z","shell.execute_reply.started":"2024-07-09T10:40:09.670086Z","shell.execute_reply":"2024-07-09T10:40:09.673902Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"rag_t5('I just discovered the course, can I still join?')","metadata":{"execution":{"iopub.status.busy":"2024-07-09T10:40:11.370620Z","iopub.execute_input":"2024-07-09T10:40:11.371489Z","iopub.status.idle":"2024-07-09T10:40:16.209287Z","shell.execute_reply.started":"2024-07-09T10:40:11.371456Z","shell.execute_reply":"2024-07-09T10:40:16.208329Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"\"<pad> Yes, even if you don't register, you're still eligible to submit the homeworks. Be aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.</s>\""},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. [Microsoft Phi-3](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct)","metadata":{}},{"cell_type":"code","source":"import torch \nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline \n\ntorch.random.manual_seed(0) ","metadata":{"execution":{"iopub.status.busy":"2024-07-09T10:55:30.147281Z","iopub.execute_input":"2024-07-09T10:55:30.147559Z","iopub.status.idle":"2024-07-09T10:55:47.254728Z","shell.execute_reply.started":"2024-07-09T10:55:30.147533Z","shell.execute_reply":"2024-07-09T10:55:47.253942Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-07-09 10:55:36.498601: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-09 10:55:36.498712: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-09 10:55:36.611533: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7b9f4b93f310>"},"metadata":{}}]},{"cell_type":"code","source":"model = AutoModelForCausalLM.from_pretrained( \n    \"microsoft/Phi-3-mini-128k-instruct\",  \n    device_map=\"cuda\",  \n    torch_dtype=\"auto\",  \n    trust_remote_code=True,  \n) \n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\") \n","metadata":{"execution":{"iopub.status.busy":"2024-07-09T10:55:58.857754Z","iopub.execute_input":"2024-07-09T10:55:58.858485Z","iopub.status.idle":"2024-07-09T10:56:48.032958Z","shell.execute_reply.started":"2024-07-09T10:55:58.858454Z","shell.execute_reply":"2024-07-09T10:56:48.032130Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/3.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9eea8a6ba0504916931af4acff5c11af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_phi3.py:   0%|          | 0.00/11.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddcf002440f54248bb2e59da5c12d587"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-128k-instruct:\n- configuration_phi3.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_phi3.py:   0%|          | 0.00/73.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdc031c315824bafa6e2453ec652ebce"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-128k-instruct:\n- modeling_phi3.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/16.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d478a9f977442f7933cf16835844b2b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9af0d10dd10d4c75bd1125c515ff5afa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6a5639cd3f14661912443e02c7fbea7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9149c5404ebc4705accdce2fb858a240"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6840d32f0d747a38fb3ce68dddda1ed"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1907a4e62ee64bf9a810723f53b64233"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/3.44k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36195524c13f49cb8bdf4b313bd955ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de1f105a1e0e47efaf0109ae48a6c0a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.94M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1265961b2be14f1fab7c3f06df2e7b2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/306 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16407b1c2d8a4428a0c103de9c01e89e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/599 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ab62e2099cd418284dc23340fdd03d0"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"pipe = pipeline( \n    \"text-generation\", \n    model=model, \n    tokenizer=tokenizer, \n) ","metadata":{"execution":{"iopub.status.busy":"2024-07-09T10:59:29.734910Z","iopub.execute_input":"2024-07-09T10:59:29.735554Z","iopub.status.idle":"2024-07-09T10:59:29.740151Z","shell.execute_reply.started":"2024-07-09T10:59:29.735522Z","shell.execute_reply":"2024-07-09T10:59:29.739157Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"messages = [ \n    {\"role\": \"system\", \"content\": \"I just heard about LLM-Zoomcamp course, can I still join.\"}\n] \n\n\ngeneration_args = { \n    \"max_new_tokens\": 500, \n    \"return_full_text\": False, \n    \"temperature\": 0.0, \n    \"do_sample\": False, \n} \n\noutput = pipe(messages, **generation_args) \nprint(output[0]['generated_text'])\n","metadata":{"execution":{"iopub.status.busy":"2024-07-09T11:01:34.274817Z","iopub.execute_input":"2024-07-09T11:01:34.275234Z","iopub.status.idle":"2024-07-09T11:01:49.029916Z","shell.execute_reply.started":"2024-07-09T11:01:34.275202Z","shell.execute_reply":"2024-07-09T11:01:49.028991Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":" I'm sorry, but I can't provide real-time updates or information on specific courses or events. However, I can guide you on how to find out if the LLM-Zoomcamp course is still available.\n\n1. Visit the official website of the course provider or the institution offering the LLM-Zoomcamp course.\n2. Look for a section on upcoming events, courses, or webinars.\n3. Check the course schedule or announcements for any information about the LLM-Zoomcamp course.\n4. If the course is still ongoing, you may be able to join by following the registration instructions provided on the website.\n5. If the course has ended, you can still contact the course provider or institution to inquire about any recordings or materials that may be available for future reference.\n\nAlternatively, you can also search for the course title or keywords related to it on search engines or social media platforms to see if there are any recent updates or announcements.\n","output_type":"stream"}]},{"cell_type":"code","source":"# prompt template with LLM- Phi3\n\ndef build_prompt_phi(query, search_results):\n    prompt_template = \"\"\"\nYou're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\nUse only the facts from the CONTEXT when answering the QUESTION.\n\nQUESTION: {question}\n\nCONTEXT: \n{context}\n\"\"\".strip()\n\n    context = \"\"\n    \n    for doc in search_results:\n        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n    \n    prompt = prompt_template.format(question=query, context=context).strip()\n    return prompt\n\ndef llm_phi(prompt):\n    messages = [ \n        {\"role\": \"system\", \"content\": prompt}\n    ] \n\n\n    generation_args = { \n        \"max_new_tokens\": 500, \n        \"return_full_text\": False, \n        \"temperature\": 0.0, \n        \"do_sample\": False, \n    } \n\n    output = pipe(messages, **generation_args) \n    return output[0]['generated_text']\n","metadata":{"execution":{"iopub.status.busy":"2024-07-09T11:08:19.331873Z","iopub.execute_input":"2024-07-09T11:08:19.332601Z","iopub.status.idle":"2024-07-09T11:08:19.341096Z","shell.execute_reply.started":"2024-07-09T11:08:19.332567Z","shell.execute_reply":"2024-07-09T11:08:19.340149Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Combining all to RAG Query\n\ndef rag_phi(query):\n    search_results = search(query)\n    prompt = build_prompt_phi(query, search_results)\n    answer = llm_phi(prompt)\n    return answer","metadata":{"execution":{"iopub.status.busy":"2024-07-09T11:08:25.943000Z","iopub.execute_input":"2024-07-09T11:08:25.943827Z","iopub.status.idle":"2024-07-09T11:08:25.948190Z","shell.execute_reply.started":"2024-07-09T11:08:25.943795Z","shell.execute_reply":"2024-07-09T11:08:25.947042Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"rag_phi('I just discovered the course, can I still join?')","metadata":{"execution":{"iopub.status.busy":"2024-07-09T11:08:56.201677Z","iopub.execute_input":"2024-07-09T11:08:56.202200Z","iopub.status.idle":"2024-07-09T11:09:06.035926Z","shell.execute_reply.started":"2024-07-09T11:08:56.202162Z","shell.execute_reply":"2024-07-09T11:09:06.034951Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"' You can still join the course even if you discover it after the start date. You are eligible to submit the homeworks, but remember to meet the deadlines for the final projects. The course will start on 15th Jan 2024 at 17h00. Before the course starts, you can install and set up all the dependencies and requirements, and familiarize yourself with the prerequisites and syllabus. You can also contribute to the course by starring the repo, sharing it with friends, and creating a PR if you see improvements can be made.'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. [Mistral-7](https://huggingface.co/docs/transformers/en/llm_tutorial)","metadata":{}},{"cell_type":"code","source":"token = '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'","metadata":{"execution":{"iopub.status.busy":"2024-07-09T13:06:53.860148Z","iopub.execute_input":"2024-07-09T13:06:53.861058Z","iopub.status.idle":"2024-07-09T13:06:53.864843Z","shell.execute_reply.started":"2024-07-09T13:06:53.861028Z","shell.execute_reply":"2024-07-09T13:06:53.863947Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login","metadata":{"execution":{"iopub.status.busy":"2024-07-09T13:07:50.681735Z","iopub.execute_input":"2024-07-09T13:07:50.682427Z","iopub.status.idle":"2024-07-09T13:07:50.690030Z","shell.execute_reply.started":"2024-07-09T13:07:50.682399Z","shell.execute_reply":"2024-07-09T13:07:50.689160Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"login(token = token)","metadata":{"execution":{"iopub.status.busy":"2024-07-09T13:08:02.388467Z","iopub.execute_input":"2024-07-09T13:08:02.388931Z","iopub.status.idle":"2024-07-09T13:08:02.483196Z","shell.execute_reply.started":"2024-07-09T13:08:02.388899Z","shell.execute_reply":"2024-07-09T13:08:02.482297Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: read).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM\nfrom transformers import AutoTokenizer\nfrom transformers.utils.quantization_config import BitsAndBytesConfig\n","metadata":{"execution":{"iopub.status.busy":"2024-07-09T13:21:23.528275Z","iopub.execute_input":"2024-07-09T13:21:23.529072Z","iopub.status.idle":"2024-07-09T13:21:23.533089Z","shell.execute_reply.started":"2024-07-09T13:21:23.529041Z","shell.execute_reply":"2024-07-09T13:21:23.532160Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"pip install -U transformers accelerate bitsandbytes sentencepiece","metadata":{"execution":{"iopub.status.busy":"2024-07-09T13:13:57.609527Z","iopub.execute_input":"2024-07-09T13:13:57.610289Z","iopub.status.idle":"2024-07-09T13:14:27.215921Z","shell.execute_reply.started":"2024-07-09T13:13:57.610256Z","shell.execute_reply":"2024-07-09T13:14:27.214797Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\nCollecting transformers\n  Downloading transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m694.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.30.1)\nCollecting accelerate\n  Downloading accelerate-0.32.1-py3-none-any.whl.metadata (18 kB)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl.metadata (2.2 kB)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (0.2.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.2)\nRequirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading transformers-4.42.3-py3-none-any.whl (9.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.3/9.3 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading accelerate-0.32.1-py3-none-any.whl (314 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.1/314.1 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes, accelerate, transformers\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 0.30.1\n    Uninstalling accelerate-0.30.1:\n      Successfully uninstalled accelerate-0.30.1\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.41.2\n    Uninstalling transformers-4.41.2:\n      Successfully uninstalled transformers-4.41.2\nSuccessfully installed accelerate-0.32.1 bitsandbytes-0.43.1 transformers-4.42.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install --upgrade bitsandbytes","metadata":{"execution":{"iopub.status.busy":"2024-07-09T13:17:42.078282Z","iopub.execute_input":"2024-07-09T13:17:42.078947Z","iopub.status.idle":"2024-07-09T13:17:54.336959Z","shell.execute_reply.started":"2024-07-09T13:17:42.078915Z","shell.execute_reply":"2024-07-09T13:17:54.335829Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Requirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.43.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\n\n# Define quantization configuration\nquantization_config = BitsAndBytesConfig()\n\n# Load model with quantization configuration\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"mistralai/Mistral-7B-v0.1\", \n    device_map=\"auto\", \n    quantization_config=quantization_config\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", padding_side=\"left\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"quantization_config = BitsAndBytesConfig(\n    bnb_4bit_compute_dtype=\"int8\", \n    bnb_4bit_quant_type=\"static\"\n)\n\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    \"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", quantization_config=quantization_config\n)\n\ntokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", padding_side=\"left\")","metadata":{"execution":{"iopub.status.busy":"2024-07-09T13:25:02.702783Z","iopub.execute_input":"2024-07-09T13:25:02.703532Z","iopub.status.idle":"2024-07-09T13:25:02.823137Z","shell.execute_reply.started":"2024-07-09T13:25:02.703501Z","shell.execute_reply":"2024-07-09T13:25:02.822035Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Unexpected exception formatting exception. Falling back to standard exception\n","output_type":"stream"},{"name":"stderr","text":"Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"/tmp/ipykernel_34/2432043499.py\", line 7, in <module>\n    model = AutoModelForCausalLM.from_pretrained(\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 563, in from_pretrained\n    model_class = _get_model_class(config, cls._model_mapping)\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 3202, in from_pretrained\n  File \"/opt/conda/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py\", line 64, in validate_environment\n    raise ImportError(\"Using `bitsandbytes` 4-bit quantization requires Accelerate: `pip install accelerate`\")\nImportError: Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2144, in showtraceback\n    stb = self.InteractiveTB.structured_traceback(\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n    return FormattedTB.structured_traceback(\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n    return VerboseTB.structured_traceback(\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n    frames.append(self.format_record(record))\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n    frame_info.lines, Colors, self.has_colors, lvals\n  File \"/opt/conda/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 792, in lines\n    return self._sd.lines\n  File \"/opt/conda/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/opt/conda/lib/python3.10/site-packages/stack_data/core.py\", line 734, in lines\n    pieces = self.included_pieces\n  File \"/opt/conda/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/opt/conda/lib/python3.10/site-packages/stack_data/core.py\", line 681, in included_pieces\n    pos = scope_pieces.index(self.executing_piece)\n  File \"/opt/conda/lib/python3.10/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/opt/conda/lib/python3.10/site-packages/stack_data/core.py\", line 660, in executing_piece\n    return only(\n  File \"/opt/conda/lib/python3.10/site-packages/executing/executing.py\", line 116, in only\n    raise NotOneValueFound('Expected one value, found 0')\nexecuting.executing.NotOneValueFound: Expected one value, found 0\n","output_type":"stream"}]},{"cell_type":"code","source":"model_inputs = tokenizer([\"A list of colors: red, blue\"], return_tensors=\"pt\").to(\"cuda\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generated_ids = model.generate(**model_inputs)\ntokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]","metadata":{},"execution_count":null,"outputs":[]}]}